{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import base64\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pywt   \n",
    "\n",
    "def w2d(img, mode='haar', level=1):\n",
    "    imArray = img\n",
    "    #Datatype conversions\n",
    "    #convert to grayscale\n",
    "    imArray = cv2.cvtColor( imArray,cv2.COLOR_RGB2GRAY )\n",
    "    #convert to float\n",
    "    imArray =  np.float32(imArray)   \n",
    "    imArray /= 255\n",
    "    # compute coefficients \n",
    "    coeffs=pywt.wavedec2(imArray, mode, level=level)\n",
    "\n",
    "    #Process Coefficients\n",
    "    coeffs_H = list(coeffs)  \n",
    "    coeffs_H[0] *= 0  \n",
    "\n",
    "    # reconstruction\n",
    "    imArray_H=pywt.waverec2(coeffs_H, mode);\n",
    "    imArray_H *= 255\n",
    "    imArray_H =  np.uint8(imArray_H)\n",
    "\n",
    "    return imArray_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "__class_name_to_number = {}\n",
    "__class_number_to_name = {}\n",
    "\n",
    "__model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def classify_image(image_base64_data, file_path=None):\n",
    "\n",
    "    imgs = get_cropped_image_if_2_eyes(file_path, image_base64_data)\n",
    "\n",
    "    result = []\n",
    "    for img in imgs:\n",
    "        scalled_raw_img = cv2.resize(img, (32, 32))\n",
    "        img_har = w2d(img, 'db1', 5)\n",
    "        scalled_img_har = cv2.resize(img_har, (32, 32))\n",
    "        combined_img = np.vstack((scalled_raw_img.reshape(32 * 32 * 3, 1), scalled_img_har.reshape(32 * 32, 1)))\n",
    "\n",
    "        len_image_array = 32*32*3 + 32*32\n",
    "\n",
    "        final = combined_img.reshape(1,len_image_array).astype(float)\n",
    "        result.append({\n",
    "            'class': class_number_to_name(__model.predict(final)[0]),\n",
    "            'class_probability': np.around(__model.predict_proba(final)*100,2).tolist()[0],\n",
    "            'class_dictionary': __class_name_to_number\n",
    "        })\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_number_to_name(class_num):\n",
    "    return __class_number_to_name[class_num]\n",
    "\n",
    "def load_saved_artifacts():\n",
    "    print(\"loading saved artifacts...start\")\n",
    "    global __class_name_to_number\n",
    "    global __class_number_to_name\n",
    "\n",
    "    with open(\"./class_dictionary.json\", \"r\") as f:\n",
    "        __class_name_to_number = json.load(f)\n",
    "        __class_number_to_name = {v:k for k,v in __class_name_to_number.items()}\n",
    "\n",
    "    global __model\n",
    "    if __model is None:\n",
    "        with open('./saved_model.pkl', 'rb') as f:\n",
    "            __model = joblib.load(f)\n",
    "    print(\"loading saved artifacts...done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cv2_image_from_base64_string(b64str):\n",
    "    '''\n",
    "    credit: https://stackoverflow.com/questions/33754935/read-a-base-64-encoded-image-from-memory-using-opencv-python-library\n",
    "    :param uri:\n",
    "    :return:\n",
    "    '''\n",
    "    encoded_data = b64str.split(',')[1]\n",
    "    nparr = np.frombuffer(base64.b64decode(encoded_data), np.uint8)\n",
    "    img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cropped_image_if_2_eyes(image_path, image_base64_data):\n",
    "    face_cascade = cv2.CascadeClassifier('./opencv/data/haarcascades/haarcascade_frontalface_default.xml')\n",
    "    eye_cascade = cv2.CascadeClassifier('./opencv/data/haarcascades/haarcascade_eye.xml')\n",
    "    \n",
    "    if image_path:\n",
    "        img = cv2.imread(image_path)\n",
    "    else:\n",
    "        img = get_cv2_image_from_base64_string(image_base64_data)\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    cropped_faces = []\n",
    "    for (x,y,w,h) in faces:\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_color = img[y:y+h, x:x+w]\n",
    "            eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "            if len(eyes) >= 2:\n",
    "                cropped_faces.append(roi_color)\n",
    "    return cropped_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_b64_test_image_for_virat():\n",
    "    with open(\"b64.txt\") as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading saved artifacts...start\n",
      "loading saved artifacts...done\n",
      "[{'class': 'ziach', 'class_probability': [9.72, 2.01, 13.15, 10.03, 65.09], 'class_dictionary': {'amrabat': 0, 'bono': 1, 'hakimi': 2, 'ounahi': 3, 'ziach': 4}}]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    load_saved_artifacts()\n",
    "\n",
    "    #print(classify_image(get_b64_test_image_for_virat(), None))\n",
    "\n",
    "    #print(classify_image(None, './testImg/MAROC_TOP_10.jpg'))\n",
    "    print(classify_image(None, './test_images\\pic1.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 23:02:57.657 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "\n",
    "st.title('My Streamlit App')\n",
    "st.write('Hello, world!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ngrok: 25%\r"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "public_url = ngrok.connect(port='8501')\n",
    "st.run_app(\"your_notebook.ipynb\", f\"--server.port=8501 --browser.serverAddress={public_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 22:46:16.128 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Lenovo\\AppData\\Roaming\\Python\\Python39\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    },
    {
     "ename": "InternalHashError",
     "evalue": "module '__main__' has no attribute '__file__'\n\nWhile caching the body of `get_base64_of_bin_file()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function get_base64_of_bin_file at 0x0000021181E9BD30>\n```\n\nPlease see the `hash_funcs` [documentation](https://docs.streamlit.io/library/advanced-features/caching#the-hash_funcs-parameter)\nfor more details.\n            ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:360\u001b[0m, in \u001b[0;36m_CodeHasher.to_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     \u001b[39m# Hash the input\u001b[39;00m\n\u001b[1;32m--> 360\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (tname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_bytes(obj, context))\n\u001b[0;32m    362\u001b[0m     \u001b[39m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     \u001b[39m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:626\u001b[0m, in \u001b[0;36m_CodeHasher._to_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39massert\u001b[39;00m code \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 626\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_file_should_be_hashed(code\u001b[39m.\u001b[39;49mco_filename):\n\u001b[0;32m    627\u001b[0m     context \u001b[39m=\u001b[39m _get_context(obj)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:402\u001b[0m, in \u001b[0;36m_CodeHasher._file_should_be_hashed\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    400\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[39mreturn\u001b[39;00m file_util\u001b[39m.\u001b[39mfile_is_in_folder_glob(\n\u001b[1;32m--> 402\u001b[0m     filepath, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_main_script_directory()\n\u001b[0;32m    403\u001b[0m ) \u001b[39mor\u001b[39;00m file_util\u001b[39m.\u001b[39mfile_in_pythonpath(filepath)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:714\u001b[0m, in \u001b[0;36m_CodeHasher._get_main_script_directory\u001b[1;34m()\u001b[0m\n\u001b[0;32m    712\u001b[0m \u001b[39m# This works because we set __main__.__file__ to the\u001b[39;00m\n\u001b[0;32m    713\u001b[0m \u001b[39m# script path in ScriptRunner.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m abs_main_path \u001b[39m=\u001b[39m pathlib\u001b[39m.\u001b[39mPath(__main__\u001b[39m.\u001b[39;49m\u001b[39m__file__\u001b[39;49m)\u001b[39m.\u001b[39mresolve()\n\u001b[0;32m    715\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39m(abs_main_path\u001b[39m.\u001b[39mparent)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module '__main__' has no attribute '__file__'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInternalHashError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m     st\u001b[39m.\u001b[39mmarkdown(page_bg_img, unsafe_allow_html\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     31\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m---> 33\u001b[0m set_png_as_page_bg(\u001b[39m\"\u001b[39;49m\u001b[39m./5559852.webp\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m     35\u001b[0m upload\u001b[39m=\u001b[39m st\u001b[39m.\u001b[39mfile_uploader(\u001b[39m'\u001b[39m\u001b[39mInsert image for classification\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mtype\u001b[39m\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mpng\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mjpg\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     36\u001b[0m c1, c2\u001b[39m=\u001b[39m st\u001b[39m.\u001b[39mcolumns(\u001b[39m2\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 18\u001b[0m, in \u001b[0;36mset_png_as_page_bg\u001b[1;34m(png_file)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mset_png_as_page_bg\u001b[39m(png_file):\n\u001b[1;32m---> 18\u001b[0m     bin_str \u001b[39m=\u001b[39m get_base64_of_bin_file(png_file) \n\u001b[0;32m     19\u001b[0m     page_bg_img \u001b[39m=\u001b[39m \u001b[39m'''\u001b[39m\n\u001b[0;32m     20\u001b[0m \u001b[39m    <style>\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[39m    .stApp \u001b[39m\u001b[39m{\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[39m    </style>\u001b[39m\n\u001b[0;32m     28\u001b[0m \u001b[39m    \u001b[39m\u001b[39m'''\u001b[39m \u001b[39m%\u001b[39m bin_str\n\u001b[0;32m     30\u001b[0m     st\u001b[39m.\u001b[39mmarkdown(page_bg_img, unsafe_allow_html\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\caching.py:715\u001b[0m, in \u001b[0;36mcache.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[39mif\u001b[39;00m show_spinner:\n\u001b[0;32m    714\u001b[0m     \u001b[39mwith\u001b[39;00m spinner(message):\n\u001b[1;32m--> 715\u001b[0m         \u001b[39mreturn\u001b[39;00m get_or_create_cached_value()\n\u001b[0;32m    716\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    717\u001b[0m     \u001b[39mreturn\u001b[39;00m get_or_create_cached_value()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\caching.py:637\u001b[0m, in \u001b[0;36mcache.<locals>.wrapped_func.<locals>.get_or_create_cached_value\u001b[1;34m()\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mnonlocal\u001b[39;00m cache_key\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m cache_key \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# Delay generating the cache key until the first call.\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39m# This way we can see values of globals, including functions\u001b[39;00m\n\u001b[0;32m    634\u001b[0m     \u001b[39m# defined after this one.\u001b[39;00m\n\u001b[0;32m    635\u001b[0m     \u001b[39m# If we generated the key earlier we would only hash those\u001b[39;00m\n\u001b[0;32m    636\u001b[0m     \u001b[39m# globals by name, and miss changes in their code or value.\u001b[39;00m\n\u001b[1;32m--> 637\u001b[0m     cache_key \u001b[39m=\u001b[39m _hash_func(non_optional_func, hash_funcs)\n\u001b[0;32m    639\u001b[0m \u001b[39m# First, get the cache that's attached to this function.\u001b[39;00m\n\u001b[0;32m    640\u001b[0m \u001b[39m# This cache's key is generated (above) from the function's code.\u001b[39;00m\n\u001b[0;32m    641\u001b[0m mem_cache \u001b[39m=\u001b[39m _mem_caches\u001b[39m.\u001b[39mget_cache(cache_key, max_entries, ttl)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\caching.py:767\u001b[0m, in \u001b[0;36m_hash_func\u001b[1;34m(func, hash_funcs)\u001b[0m\n\u001b[0;32m    756\u001b[0m update_hash(\n\u001b[0;32m    757\u001b[0m     (func\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m, func\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m),\n\u001b[0;32m    758\u001b[0m     hasher\u001b[39m=\u001b[39mfunc_hasher,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    761\u001b[0m     hash_source\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m    762\u001b[0m )\n\u001b[0;32m    764\u001b[0m \u001b[39m# Include the function's body in the hash. We *do* pass hash_funcs here,\u001b[39;00m\n\u001b[0;32m    765\u001b[0m \u001b[39m# because this step will be hashing any objects referenced in the function\u001b[39;00m\n\u001b[0;32m    766\u001b[0m \u001b[39m# body.\u001b[39;00m\n\u001b[1;32m--> 767\u001b[0m update_hash(\n\u001b[0;32m    768\u001b[0m     func,\n\u001b[0;32m    769\u001b[0m     hasher\u001b[39m=\u001b[39;49mfunc_hasher,\n\u001b[0;32m    770\u001b[0m     hash_funcs\u001b[39m=\u001b[39;49mhash_funcs,\n\u001b[0;32m    771\u001b[0m     hash_reason\u001b[39m=\u001b[39;49mHashReason\u001b[39m.\u001b[39;49mCACHING_FUNC_BODY,\n\u001b[0;32m    772\u001b[0m     hash_source\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    773\u001b[0m )\n\u001b[0;32m    774\u001b[0m cache_key \u001b[39m=\u001b[39m func_hasher\u001b[39m.\u001b[39mhexdigest()\n\u001b[0;32m    775\u001b[0m _LOGGER\u001b[39m.\u001b[39mdebug(\n\u001b[0;32m    776\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mmem_cache key for \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, func\u001b[39m.\u001b[39m\u001b[39m__module__\u001b[39m, func\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m, cache_key\n\u001b[0;32m    777\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:108\u001b[0m, in \u001b[0;36mupdate_hash\u001b[1;34m(val, hasher, hash_reason, hash_source, context, hash_funcs)\u001b[0m\n\u001b[0;32m    105\u001b[0m hash_stacks\u001b[39m.\u001b[39mcurrent\u001b[39m.\u001b[39mhash_source \u001b[39m=\u001b[39m hash_source\n\u001b[0;32m    107\u001b[0m ch \u001b[39m=\u001b[39m _CodeHasher(hash_funcs)\n\u001b[1;32m--> 108\u001b[0m ch\u001b[39m.\u001b[39;49mupdate(hasher, val, context)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:385\u001b[0m, in \u001b[0;36m_CodeHasher.update\u001b[1;34m(self, hasher, obj, context)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mupdate\u001b[39m(\u001b[39mself\u001b[39m, hasher, obj: Any, context: Optional[Context] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    384\u001b[0m     \u001b[39m\"\"\"Update the provided hasher with the hash of an object.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 385\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mto_bytes(obj, context)\n\u001b[0;32m    386\u001b[0m     hasher\u001b[39m.\u001b[39mupdate(b)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:374\u001b[0m, in \u001b[0;36m_CodeHasher.to_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    371\u001b[0m     \u001b[39mraise\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m ex:\n\u001b[1;32m--> 374\u001b[0m     \u001b[39mraise\u001b[39;00m InternalHashError(ex, obj)\n\u001b[0;32m    376\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    377\u001b[0m     \u001b[39m# In case an UnhashableTypeError (or other) error is thrown, clean up the\u001b[39;00m\n\u001b[0;32m    378\u001b[0m     \u001b[39m# stack so we don't get false positives in future hashing calls\u001b[39;00m\n\u001b[0;32m    379\u001b[0m     hash_stacks\u001b[39m.\u001b[39mcurrent\u001b[39m.\u001b[39mpop()\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:360\u001b[0m, in \u001b[0;36m_CodeHasher.to_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    356\u001b[0m hash_stacks\u001b[39m.\u001b[39mcurrent\u001b[39m.\u001b[39mpush(obj)\n\u001b[0;32m    358\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     \u001b[39m# Hash the input\u001b[39;00m\n\u001b[1;32m--> 360\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (tname, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_bytes(obj, context))\n\u001b[0;32m    362\u001b[0m     \u001b[39m# Hmmm... It's possible that the size calculation is wrong. When we\u001b[39;00m\n\u001b[0;32m    363\u001b[0m     \u001b[39m# call to_bytes inside _to_bytes things get double-counted.\u001b[39;00m\n\u001b[0;32m    364\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mgetsizeof(b)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:626\u001b[0m, in \u001b[0;36m_CodeHasher._to_bytes\u001b[1;34m(self, obj, context)\u001b[0m\n\u001b[0;32m    624\u001b[0m code \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__code__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    625\u001b[0m \u001b[39massert\u001b[39;00m code \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 626\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_file_should_be_hashed(code\u001b[39m.\u001b[39;49mco_filename):\n\u001b[0;32m    627\u001b[0m     context \u001b[39m=\u001b[39m _get_context(obj)\n\u001b[0;32m    628\u001b[0m     defaults \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m__defaults__\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:402\u001b[0m, in \u001b[0;36m_CodeHasher._file_should_be_hashed\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m file_is_blacklisted:\n\u001b[0;32m    400\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[39mreturn\u001b[39;00m file_util\u001b[39m.\u001b[39mfile_is_in_folder_glob(\n\u001b[1;32m--> 402\u001b[0m     filepath, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_main_script_directory()\n\u001b[0;32m    403\u001b[0m ) \u001b[39mor\u001b[39;00m file_util\u001b[39m.\u001b[39mfile_in_pythonpath(filepath)\n",
      "File \u001b[1;32mc:\\Users\\Lenovo\\anaconda3\\lib\\site-packages\\streamlit\\runtime\\legacy_caching\\hashing.py:714\u001b[0m, in \u001b[0;36m_CodeHasher._get_main_script_directory\u001b[1;34m()\u001b[0m\n\u001b[0;32m    710\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39m__main__\u001b[39;00m\n\u001b[0;32m    712\u001b[0m \u001b[39m# This works because we set __main__.__file__ to the\u001b[39;00m\n\u001b[0;32m    713\u001b[0m \u001b[39m# script path in ScriptRunner.\u001b[39;00m\n\u001b[1;32m--> 714\u001b[0m abs_main_path \u001b[39m=\u001b[39m pathlib\u001b[39m.\u001b[39mPath(__main__\u001b[39m.\u001b[39;49m\u001b[39m__file__\u001b[39;49m)\u001b[39m.\u001b[39mresolve()\n\u001b[0;32m    715\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mstr\u001b[39m(abs_main_path\u001b[39m.\u001b[39mparent)\n",
      "\u001b[1;31mInternalHashError\u001b[0m: module '__main__' has no attribute '__file__'\n\nWhile caching the body of `get_base64_of_bin_file()`, Streamlit encountered an\nobject of type `builtins.function`, which it does not know how to hash.\n\n**In this specific case, it's very likely you found a Streamlit bug so please\n[file a bug report here.]\n(https://github.com/streamlit/streamlit/issues/new/choose)**\n\nIn the meantime, you can try bypassing this error by registering a custom\nhash function via the `hash_funcs` keyword in @st.cache(). For example:\n\n```\n@st.cache(hash_funcs={builtins.function: my_hash_func})\ndef my_func(...):\n    ...\n```\n\nIf you don't know where the object of type `builtins.function` is coming\nfrom, try looking at the hash chain below for an object that you do recognize,\nthen pass that to `hash_funcs` instead:\n\n```\nObject of type builtins.function: <function get_base64_of_bin_file at 0x0000021181E9BD30>\n```\n\nPlease see the `hash_funcs` [documentation](https://docs.streamlit.io/library/advanced-features/caching#the-hash_funcs-parameter)\nfor more details.\n            "
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import base64\n",
    "from PIL import Image\n",
    "\n",
    "st.markdown('<h1 style=\"color:black;\">Vgg 19 Image classification model</h1>', unsafe_allow_html=True)\n",
    "st.markdown('<h2 style=\"color:gray;\">The image classification model classifies image into following categories:</h2>', unsafe_allow_html=True)\n",
    "st.markdown('<h3 style=\"color:gray;\"> street,  buildings, forest, sea, mountain, glacier</h3>', unsafe_allow_html=True)\n",
    "\n",
    "# background image to streamlit\n",
    "\n",
    "@st.cache(allow_output_mutation=True)\n",
    "def get_base64_of_bin_file(bin_file):\n",
    "    with open(bin_file, 'rb') as f:\n",
    "        data = f.read()\n",
    "    return base64.b64encode(data).decode()\n",
    "\n",
    "def set_png_as_page_bg(png_file):\n",
    "    bin_str = get_base64_of_bin_file(png_file) \n",
    "    page_bg_img = '''\n",
    "    <style>\n",
    "    .stApp {\n",
    "    background-image: url(\"data:image/png;base64,%s\");\n",
    "    background-size: cover;\n",
    "    background-repeat: no-repeat;\n",
    "    background-attachment: scroll; # doesn't work\n",
    "    }\n",
    "    </style>\n",
    "    ''' % bin_str\n",
    "    \n",
    "    st.markdown(page_bg_img, unsafe_allow_html=True)\n",
    "    return\n",
    "\n",
    "set_png_as_page_bg(\"./5559852.webp\")\n",
    "\n",
    "upload= st.file_uploader('Insert image for classification', type=['png','jpg'])\n",
    "c1, c2= st.columns(2)\n",
    "if upload is not None:\n",
    "  url = upload.url\n",
    "  im= Image.open(upload)\n",
    "  img= np.asarray(im)\n",
    "  image= cv2.resize(img,(224, 224))\n",
    "  #img= preprocess_input(image)\n",
    "  img= np.expand_dims(img, 0)\n",
    "  c1.header('Input Image')\n",
    "  c1.image(im)\n",
    "  #c1.write(img.shape)\n",
    "\n",
    "  # prediction on model\n",
    "  c2.header('Output')\n",
    "  c2.subheader('Predicted class :')\n",
    "  c2.write(classify_image(None, url) )\n",
    "'''\n",
    "   #load weights of the trained model.\n",
    "  input_shape = (224, 224, 3)\n",
    "  optim_1 = Adam(learning_rate=0.0001)\n",
    "  n_classes=6\n",
    "  vgg_model = model(input_shape, n_classes, optim_1, fine_tune=2)\n",
    "  vgg_model.load_weights('/content/drive/MyDrive/vgg/tune_model19.weights.best.hdf5') \n",
    "  \n",
    "  vgg_preds = vgg_model.predict(img)\n",
    "  vgg_pred_classes = np.argmax(vgg_preds, axis=1)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
